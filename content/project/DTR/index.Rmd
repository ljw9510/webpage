---
title: "Review of 'Q- and A-learning Methods for Estimating Optimal Dynamic Treatment Regimes'"
subtitle: Phillip J. Schulte, Anastasios A. Tsiatis, Eric B. Laber, and Marie Davidian
author: "Joowon Lee"
date: "2022-10-24"
categories: ["Causal Inference"]
tags: ["Causal Inference", "Dynamic Treatment Regime"]
excerpt: Review of the paper "Q- and A-learning Methods for Estimating Optimal Dynamic Treatment Regimes" written by P. J. Schulte, et al., (2014).
links:
- icon: door-open
  icon_pack: fas
  name: paper
  url: https://pubmed.ncbi.nlm.nih.gov/25620840/
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

---

### List

[1. Overview](#overview)

[2. Identification](#identification)


---


### Overview

  In this project, we will review a paper *Q- and A-learning Methods for Estimating Optimal Dynamic Treatment Regimes* written by Phillip J. Schulte et al., (2014) [[paper]](https://pubmed.ncbi.nlm.nih.gov/25620840/).


  In clinical practice, clinicians make a series of decisions based on individual patient's baseline and evolving characteristics. In this paper, we wanted to make optimal sequential treatment decisions to achieve the 'best' outcome for an individual patient. The list of sequential decision rules is formalized as a dynamic treatment regime. Throughout this paper, we investigate two main methods, Q-learning and A-learning, to estimate the optimal dynamic treatment regime based on data from a clinical trial or observational study.

  Let us first introduce the following notations:
  
> * $k$ : stages or pre-specified ordered decision points $k = 1, \cdots, K$,
> * $\Omega$ : a superpopulation of patients with a patient $\omega \in \Omega$,
> * $Y$ : a final outcome of interest (assuming that large values are preferred),
> * $X_{k}$ : covariate information immediately prior to decision $k$ with value $x_{k} \in \mathcal{X}_{k}$,
> * $A_{k}$ : treatment at stage $k$ with value $a_{k} \in \mathcal{A}_{k}$ (a finite set possible options) 
>  + $\overline{a_{k}} = (a_{1}, \cdots, a_{k})$  : a possible treatment history through decision $k$ taking values in $\overline{\mathcal{A}_{k}} = \mathcal{A}_{1} \times \cdots \mathcal{A}_{k}$.
          
>Then we define the **potential outcomes**
>  \begin{align*}
>  W^{*} &= \left\{ X^{*}_{2}(a_{1}), X^{*}_{3}\big(\overline{a_{2}}\big), \cdots, X^{*}_{K}\big(\overline{a_{k-1}}\big), Y^{*}\big(\overline{a_{k}}\big) \,\, \text{ for all } \, \overline{a_{k}} \in \overline{\mathcal{A}_{k}} \right\} 
>  \end{align*}
  
  Here, $X^{*}_{K}\big(\overline{a_{k-1}}\big)(\omega)$ denotes the value of covariate information that would arise between decisions $k-1$ and $k$ for a patient $\omega \in \Omega$ who have received treatment history $\overline{a_{k-1}}$, taking value $x_{k}$ in a set $\mathcal{X}_{k}$. Similarly, $Y^{*}\big(\overline{a_{k}}\big)$ is the hypothetical outcome that would result for a patient $\omega$ who have received the full set of $K$ treatment in $\overline{a_{k}}$. For simplicity, we write $\overline{X}^{*}_{\underline{k}}\big(\overline{a_{k-1}}\big)=\{X_{1},  X^{*}_{2}(a_{1}), \cdots, X^{*}_{K}\big(\overline{a_{k-1}}\big)\}, k = 1, \cdots, K$.

---

### Dynamic Treatment Regime

A dynamic treatment regime $\mathbf{d} = (d_{1}, \cdots, d_{K})$ is a set of rules that forms an algorithm to treat individual patient over time (it is called '*dynamic*' because treatment is determined based on a patient's previous history). For example, at the $k$th stage, 

* the $k$th rule $d_{k}$ takes
  + input : $\overline{x_{k}}, \overline{a_{k-1}}$ (patient's realized covariate, and treatment history up to $k-1$th decision),
  + output : $a_{k} \in \Psi_{k}(\overline{x_{k}}, \overline{a_{k-1}}) \subset \mathcal{A}_{k}$ where $\Psi_{k}(\overline{x_{k}}, \overline{a_{k-1}})$ is a pre-specified set of possible traetment options for a patient with ($\overline{x_{k}}, \overline{a_{k-1}}$).
  
Since $d_{k}$ need only map a subset of $\overline{\mathcal{X}_{k}} \times \overline{\mathcal{A}_{k-1}}$, namely $\Gamma_{k}$, to $\mathcal{A}_{k}$, let us define the subset **recursively** as

\begin{align*}
\Gamma_{k} &= \{ (\overline{x_{k}}, \overline{a_{k-1}}) \in \overline{\mathcal{X}_{k}} \times \overline{\mathcal{A}_{k-1}} \,\, \text{ satisfying } \\
& \quad \quad \text{(i) } \, a_{j} \in \Psi_{j}\big( \overline{x_{j}}, \overline{a_{j-1}} \big), \,\, j = 1, \cdots, k-1, \\
& \quad \quad \text{(ii) } \, \mathbb{P}(\overline{X_{k}}\big(\overline{a_{k-1}} \big)=\overline{x_{k}})>0\},
\end{align*}

$k=1, \cdots, K$, determined by $\Psi=(\Psi_{1}, \cdots, \Psi_{K})$. Define the class $\mathcal{D}$ of dynamic treatment regimes to be the set of all $\mathbf{d}=(d_{1}, \cdots. d_{k})$ where $d_{k}: \Gamma_{k} \rightarrow \mathcal{A}_{k}, k=1, \cdots, K$. 

>Then, for any $\mathbf{d} \in \mathcal{D}$, define the **potential outcomes associated with $\mathbf{d}$** as 
>\begin{align*}
\left\{ X^{*}_{2}(d_{1}),  \cdots, X^{*}_{k}\big(\overline{d_{k-1}}\big), \cdots,  X^{*}_{K}\big(\overline{d_{k-1}}\big), Y^{*}\big(\mathbf{d}\big) \,\, \right\}
\end{align*}

Then how can we find an optimal regime (the 'best' treatment for a patient) with these definitions?

The expected outcome in the population with all patients who have a baseline covariate $X_{1} = x_{1}$ and follow regime $\mathbf{d}$ is $\mathbb{E}[Y^{*}\big(\mathbf{d}\big)\ | X_{1} = x_{1}]$. Then an optimal regime, $\mathbf{d}^{opt} \in \mathcal{D}$, should satisfies

\begin{align*}
& \mathbb{E}[Y^{*}\big(\mathbf{d}\big)\ | X_{1} = x_{1}] \quad \le \quad \mathbb{E}[Y^{*}\big(\mathbf{d}^{opt}\big)\ | X_{1} = x_{1}], \,\, \text{ for all } \mathbf{d} \in \mathcal{D} \text{ and all } x_{1} \in \mathcal{X}_{1} \\
\Rightarrow \,\, & \mathbb{E}[Y^{*}\big(\mathbf{d}\big)] \qquad \qquad \quad \, \le \quad \mathbb{E}[Y^{*}\big(\mathbf{d}^{opt}\big)], \qquad \quad \quad \,\, \text{ for all } \mathbf{d} \in \mathcal{D}
\end{align*}

---

### Model Setup and Assumptions {#Model_Setup_and_Assumptions}

The problem is that potential outcomes for an individual patient for all $\mathbf{d} \in \mathcal{D}$ may not be observed. Therefore, the goal of dynamic treatment regime is to estimate $\mathbf{d}^{opt}$ based on data. Consider a study with a random sample of $n$ patients in $\Omega$. Assume independent and identically distributed time-ordered random variables $(X_{1i}, A_{1i}, \cdots, X_{Ki}, A_{Ki}, Y_{i}), i = 1, \cdots, n$.

Use causal framework to estimate an optimal regime from the observed data under the following assumptions:

* Consistency assumption : $X_{k} = X^{*}_{k}\big(\overline{A_{k-1}}\big), k=2, \cdots, K$ and $Y=Y^{*}\big(\overline{A_{k}}\big)$,
* Stable unit treatment value assumption (well-defineness) : a patient's covariates and outcome are not affected by treatment assignment and other patients,
* Sequential Ignorability : $A_{k} \perp W^{*} | \{\overline{X_{k}}, \overline{A_{k-1}} \}, k=1, \cdots, K$.

---

### Identification

In this section, we are interested in identifying the expected potential outcome $\mathbb{E}[Y^{*}(\overline{a_{K}})]$ which may or may not be observable based on statistical quantity.

We first consider a fixed treatment $(a_{1}, a_{2}) \in \{0, 1\}^{2}$ with the two time points $(k = 2)$. We have time-ordered variables $(X_{1}, A_{1}, X_{2}, A_{2}, Y)$, where the dependency between them can be described as the following dynamic acyclic graph (DAG):

```{r dag, fig.cap="Dynamic acyclic graph for k = 2", fig.align = 'center', , out.width = '50%', echo = F}
knitr::include_graphics("figures/dag.png")
```

From the following lemma below, we show that the expected potential outcome  $\mathbb{E}[Y^{*}(\overline{a_{2}})]=\mathbb{E}[Y^{*}(a_{1}, a_{2})]$ of treatment $(a_{1}, a_{2})$ can be identified as a statistical estimand, a function of conditional expectation.

\

\newtheorem{lemma}[theorem]{Lemma}

::: {.lemma}
*Fix a treatment $(a_{1},a_{2})\in \{0,1\}^{2}$. Under the assumptions of consistency, SUTVA, and sequential ignorability mentioned in the section [Model setup and assumptions](#Model_Setup_and_Assumptions), we have*
\begin{align*}
&\mathbb{E}[Y^{*}(a_1, a_2)] \\
& \quad = \mathbb{E}_{X_{1}} \bigg[ \mathbb{E}_{X_{2}} \bigg[ \mathbb{E}_{Y}[Y | X_{1}, A_{1} = a_{1}, X_{2}, A_{2} = a_{2}] \bigg| X_{1}, A_{1} = a_{1} \bigg] \bigg] \\
& \quad = \int_{x_{1}} \int_{x_{2}} \mathbb{E}_{Y}[Y | X_{1}, A_{1} = a_{1}, X_{2}, A_{2} = a_{2}] \,\, \mathbb{P}\left(X_{2}=x_{2}|X_{1}=x_{1}, A_{1} = a_{1} \right) \mathbb{P}\left(X_{1}=x_{1}\right)dx_{2}dx_{1}.
\end{align*}
:::

::: {.proof}
${}$
Here, let us denote $X^{*}_{2} := X^{*}_{2}(a_{1})$ and $Y^{*} := Y^{*}(a_1, a_2)$, which we will be using when these random variables are in subscripts for simplicity.  Then
\begin{align*}
&\mathbb{E}[Y^{*}(a_1, a_2)] \\
&\quad =\mathbb{E}_{X_{1}} \bigg[ \mathbb{E}_{X^{*}_{2},Y^{*}}[Y^{*}(a_1, a_2) | X_{1}]\bigg] \\
&\quad \overset{(a)}{=}\mathbb{E}_{X_{1}} \bigg[ \mathbb{E}_{X^{*}_{2},Y^{*}}[Y^{*}(a_1, a_2) | X_{1}, A_{1} = a_{1}]\bigg] \\
&\quad \overset{(b)}{=}\mathbb{E}_{X_{1}} \bigg[ \mathbb{E}_{X^{*}_{2}} \bigg[ \mathbb{E}_{Y^{*}}[Y^{*}(a_1, a_2) | X_{1}, A_{1} = a_{1}, X^{*}_{2}(a_{1})] \bigg| X_{1}, A_{1} = a_{1}\bigg] \bigg] \\
&\quad \overset{(c)}{=}\mathbb{E}_{X_{1}} \bigg[ \mathbb{E}_{X^{*}_{2}} \bigg[ \mathbb{E}_{Y^{*}}[Y^{*}(a_1, a_2) | X_{1}, A_{1} = a_{1}, X^{*}_{2}(a_{1}), A_{2} = a_{2}] \bigg| X_{1}, A_{1} = a_{1}\bigg] \bigg]  \\
&\quad \overset{(d)}{=}\mathbb{E}_{X_{1}} \bigg[ \mathbb{E}_{X_{2}} \bigg[ \mathbb{E}_{Y}[Y| X_{1}, A_{1} = a_{1}, X_{2}, A_{2} = a_{2}] \bigg| X_{1}, A_{1} = a_{1}\bigg] \bigg] \\
&\quad =  \int_{x_{1}} \int_{x_{2}} \mathbb{E}[Y | X_{1} = x_{1}, A_{1} = a_{1}, X_{2} = x_{2}, A_{2} = a_{2}] \mathbb{P}\left(X_{2} = x_{2} | X_{1} = x_{1}, A_{1} = a_{1}\right) \mathbb{P}(X_{1} = x_{1}) dx_{2} dx_{1}, \\
\end{align*}
where (a) and (c) use sequential ignorability, (b) uses iterated expectation, and (d) use consistency.
\begin{align*}
\hspace{15cm} \square
\end{align*}
:::

Now consider the general case when we have $K$ time points. We have time-varying variables $(X_{1}, A_{1}, \cdots, X_{K}, A_{K}, Y)$, assuming all dependencies between variables with proper temporal orders as shown in Figure \@ref(fig:dag). Then we introduce the lemma for identification with $K$ time points.

\

::: {.lemma}
 *Fix a treatment $\overline{a_{K}}=(a_{1},\cdots,a_{K})\in \{0,1\}^{K}$. Under the assumptions of consistency, SUTVA, and sequential ignorability, we have*
\begin{align*}
&\mathbb{E}[Y^{*}(\overline{a_{K}})] \\
&\quad = \mathbb{E}_{X_{1}} \bigg[ \mathbb{E}_{X_{2}} \bigg[ \cdots 
\bigg[ \mathbb{E}_{X_{K}}\left[ \mathbb{E}\left[Y \bigg| \begin{matrix} \overline{X_{K}} \\ \overline{A_{K}} = \overline{a_{K}} \end{matrix}\right]
\bigg| \begin{matrix} \overline{X_{K-1}} \\ \overline{A_{k-1}} = \overline{a_{k-1}} \end{matrix} \right]  
\bigg| \begin{matrix} \overline{X_{K-2}} \\ \overline{A_{k-2}} = \overline{a_{k-2}} \end{matrix} \bigg] \cdots 
\bigg| \begin{matrix} X_{1} \\ A_{1} = a_{1} \end{matrix}\bigg] \bigg]
\end{align*}
:::

::: {.proof}
${}$

The idea of the proof is to use induction on the number of unobserved treatments. Let us define the following quantities:
\begin{align*}
\mathcal{F}_{k} &: \sigma \text{-algebra generated by information up to } k , \\
\overline{Z_{[k+1,K]}} &= (Z_{k+1}, Z_{k+2}, \cdots, Z_{K}).
\end{align*}
We prove the following stronger claim:

For $k \in \{0, 1, \cdots, K \}$,
\begin{align*}
&\mathbb{E}\left[ Y^{*}(\overline{A_{k}}, \overline{a_{[k+1,K]}}) \bigg|\mathcal{F}_{k}\right] 
= \mathbb{E}_{X_{k+1}}\left[ \mathbb{E}_{X_{k+2}}
\left[ \cdots \left[ \mathbb{E}\left[Y \bigg| \begin{matrix} \overline{X_{[k+1, K]}} \\ \overline{A_{[k+1, K]}} = \overline{a_{[k+1,T]}} \\ \mathcal{F}_{k} \end{matrix}\right] \bigg| \right] \cdots \right] 
\bigg| \begin{matrix} \overline{X_{k+1}} \\ \overline{A_{k+1}} = \overline{a_{k+1}} \\ \mathcal{F}_{k} \end{matrix}
\right]
\end{align*}
Then the statement follows by taking $k=0$. We use induction on $K-k=$ the number of unobserved treatment.

First, Let $k = K$. Since we have all information up to K and there is no counterfactual outcome,
\begin{align*}
& \mathbb{E}[Y^{*}(A_{1}, \cdots, A_{K})|\mathcal{F}_{K}]  = \mathbb{E}[Y|\overline{X_{K}}, \overline{A_{K}}, \mathcal{F}_{K}]. \\
\end{align*}

Next, assume induction hypothesis for $k$ holds. We wish to show induction hypothesis holds for $k-1$.
\begin{align*}
& \mathbb{E}[Y^{*}(A_{1}, \cdots, A_{k-1}, a_{k}, \cdots, a_{K}) | \mathcal{F}_{k-1}] \\
& \quad \overset{(a)}{=} \mathbb{E}_{X_{k}} \left[ \mathbb{E} \left[ Y^{*}(A_{1}, \cdots, A_{k-1}, a_{k}, \cdots, a_{K}) \bigg| X_{k}, \mathcal{F}_{k-1} \right] \right] \\
& \quad \overset{(b)}{=} \mathbb{E}_{X_{k}} \left[ \mathbb{E} \left[ Y^{*}(A_{1}, \cdots, A_{k-1}, a_{k}, \cdots, a_{K}) \bigg| X_{k}, \overline{A_{k}} = \overline{a_{k}}, \mathcal{F}_{k-1} \right] \right] \\
& \quad = \mathbb{E}_{X_{k}} \left[ \mathbb{E} \left[ Y^{*}(A_{1}, \cdots, A_{k-1}, A_{k}, \cdots, a_{K}) \bigg| X_{k}, \overline{A_{k}} = \overline{a_{k}}, \mathcal{F}_{k-1} \right] \right] \\
& \quad \overset{(c)}{=} \mathbb{E}_{X_{k}} \left[ \mathbb{E} \left[ Y^{*}(A_{1}, \cdots, A_{k-1}, A_{k}, \cdots, a_{K}) \bigg| \mathcal{F}_{k} \right] \right] \\
& \quad = \mathbb{E}_{X_{k}} \left[ \mathbb{E} \left[ Y^{*}(A_{1}, \cdots, A_{k}, a_{k+1}, \cdots, a_{K}) \bigg|  \mathcal{F}_{k} \right] \right] \\
& \quad \overset{(d)}{=} \mathbb{E}_{X_{k}} \left[ \mathbb{E}_{X_{k+1}}\left[ \mathbb{E}_{X_{k+2}}
\left[ \cdots \left[ \mathbb{E}\left[Y \bigg| \begin{matrix} \overline{X_{[k+1, K]}} \\ \overline{A_{[k+1, K]}} = \overline{a_{[k+1,T]}} \\ \mathcal{F}_{k} \end{matrix}\right] \bigg| \right] \cdots \right] 
\bigg| \begin{matrix} \overline{X_{k+1}} \\ \overline{A_{k+1}} = \overline{a_{k+1}} \\ \mathcal{F}_{k} \end{matrix}
\right] \right]
\end{align*}
where (a) uses iterated expectation, (b) uses sequential ignorability, (c) uses no unmeasured confounder assumption in SUTVA, and (d) is from the assumed induction hypothesis. Since it holds for $k-1$, we are done.
\begin{align*}
\hspace{12.5cm} \square
\end{align*}
:::
